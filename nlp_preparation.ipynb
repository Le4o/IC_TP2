{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "Convertendo o arquivo csv em um Dataframe para preparação da análise de dados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/le4o/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/le4o/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/le4o/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /home/le4o/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Download nlkt packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "0      1956967341       empty   \n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "...           ...         ...   \n",
       "39995  1753918954     neutral   \n",
       "39996  1753919001        love   \n",
       "39997  1753919005        love   \n",
       "39998  1753919043   happiness   \n",
       "39999  1753919049        love   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>wants to hang out with friends SOON!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>@JohnLloydTaylor</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>Happy Mothers Day  All my love</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "columns = ['tweet_id', 'sentiment', 'content']\n",
    "\n",
    "# Seed para os resultados da tokenização\n",
    "np.random.seed(500)\n",
    "\n",
    "# skip_blank_lines remove as possíveis linhas em branco\n",
    "data = pd.read_csv(r'data/tweet_emotions.csv', skip_blank_lines=True)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_to_lowercase(x): return str(x).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "0      1956967341       empty   \n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "...           ...         ...   \n",
       "39995  1753918954     neutral   \n",
       "39996  1753919001        love   \n",
       "39997  1753919005        love   \n",
       "39998  1753919043   happiness   \n",
       "39999  1753919049        love   \n",
       "\n",
       "                                                 content  \\\n",
       "0      [@, tiffanylue, i, know, i, was, listenin, to,...   \n",
       "1      [layin, n, bed, with, a, headache, ughhhh, ......   \n",
       "2          [funeral, ceremony, ..., gloomy, friday, ...]   \n",
       "3         [wants, to, hang, out, with, friends, soon, !]   \n",
       "4      [@, dannycastillo, we, want, to, trade, with, ...   \n",
       "...                                                  ...   \n",
       "39995                               [@, johnlloydtaylor]   \n",
       "39996               [happy, mothers, day, all, my, love]   \n",
       "39997  [happy, mother, 's, day, to, all, the, mommies...   \n",
       "39998  [@, niariley, wassup, beautiful, !, !, !, foll...   \n",
       "39999  [@, mopedronin, bullet, train, from, tokyo, th...   \n",
       "\n",
       "                                              text_final  \n",
       "0      ['tiffanylue', 'know', 'listenin', 'bad', 'hab...  \n",
       "1      ['layin', 'n', 'bed', 'headache', 'ughhhh', 'w...  \n",
       "2            ['funeral', 'ceremony', 'gloomy', 'friday']  \n",
       "3                     ['want', 'hang', 'friend', 'soon']  \n",
       "4      ['dannycastillo', 'want', 'trade', 'someone', ...  \n",
       "...                                                  ...  \n",
       "39995                                ['johnlloydtaylor']  \n",
       "39996                 ['happy', 'mother', 'day', 'love']  \n",
       "39997  ['happy', 'mother', 'day', 'mommy', 'woman', '...  \n",
       "39998  ['niariley', 'wassup', 'beautiful', 'follow', ...  \n",
       "39999  ['mopedronin', 'bullet', 'train', 'tokyo', 'gf...  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>content</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>[@, tiffanylue, i, know, i, was, listenin, to,...</td>\n      <td>['tiffanylue', 'know', 'listenin', 'bad', 'hab...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>[layin, n, bed, with, a, headache, ughhhh, ......</td>\n      <td>['layin', 'n', 'bed', 'headache', 'ughhhh', 'w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>[funeral, ceremony, ..., gloomy, friday, ...]</td>\n      <td>['funeral', 'ceremony', 'gloomy', 'friday']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>[wants, to, hang, out, with, friends, soon, !]</td>\n      <td>['want', 'hang', 'friend', 'soon']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>[@, dannycastillo, we, want, to, trade, with, ...</td>\n      <td>['dannycastillo', 'want', 'trade', 'someone', ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>[@, johnlloydtaylor]</td>\n      <td>['johnlloydtaylor']</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>[happy, mothers, day, all, my, love]</td>\n      <td>['happy', 'mother', 'day', 'love']</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>[happy, mother, 's, day, to, all, the, mommies...</td>\n      <td>['happy', 'mother', 'day', 'mommy', 'woman', '...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>[@, niariley, wassup, beautiful, !, !, !, foll...</td>\n      <td>['niariley', 'wassup', 'beautiful', 'follow', ...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>[@, mopedronin, bullet, train, from, tokyo, th...</td>\n      <td>['mopedronin', 'bullet', 'train', 'tokyo', 'gf...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Removendo linhas em branco\n",
    "df = df.drop(df[(df.tweet_id == 0) | (df.sentiment == \"\") | (df.content == \"\")].index)\n",
    "\n",
    "# Convertendo txto em caixa alta para caixa baixa\n",
    "df['content'] = df['content'].apply(uppercase_to_lowercase)\n",
    "\n",
    "# Criando os tokens para o texto do tweet \n",
    "df['content'] = [word_tokenize(text) for text in df['content']]\n",
    "\n",
    "# WordNetLemmatizer requer tags para entender se uma palavra é um adjetivo, verbo ou um nome.\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['content']):\n",
    "    # Declarando uma lista vazia para salvar as palavras que seguem as regras para esta etapa\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    # A função pos_tag abaixo classificará a palavra\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # Salvando o resultado final em text_final\n",
    "    df.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separando os dados para teste e treino\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['text_final'], df['sentiment'], test_size=0.3)\n",
    "\n",
    "# Transformando os dados de target (sentiments) em um dataset numérico \n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "# Transformando a coleção dos dados dos tweets em um dataset numérico\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "dataset_ob = {\n",
    "    'train_x': Train_X,\n",
    "    'test_x': Test_X,\n",
    "    'train_y': Train_Y,\n",
    "    'test_y': Test_Y,\n",
    "    'tfidf_vect': Tfidf_vect,\n",
    "    'train_x_tfidf': Train_X_Tfidf,\n",
    "    'test_x_tfidf': Test_X_Tfidf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando o pickle para carregar os objetos para os outros notebooks\n",
    "with open('./data/dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_ob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}